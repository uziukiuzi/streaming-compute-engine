# Streaming Compute Engine
This is a Java framework which can be used to run arbitrary distributed tasks on a cluster, much like Hadoop MapReduce, but updating results as a real-time stream rather than as a batch process. There are many conceivable applications in a wide range of domains, for example in computational fluid dynamics, meteorology or simulated surgery. All source is provided here as well as a basic sample program which runs a Simpson's Rule integration on an arbitrary number of nodes. The remainder of this description details the major components of the code and how they work as well as discussing the project's contributions, limitations and planned extensions.

The framework functions as a master-worker model. The client program sends data to the master which distributes it in chunks to workers in a round-robin manner. The number of data points in each chunk is specified in advance. As each worker produces result units, one per chunk, it streams these back to the master which sequences the incoming units on the fly using a construct we will refer to as a 'chain space'. The Java Reflection API is used to run dynamically downloaded user code on worker machines.

The starting points are the classes ComputeClient, ComputeMaster and ComputeWorker, which should each be run on separate machines (or on separate processes in the same machine for testing purposes). The ComputeClient sends a batch of indexed data (each data point could be of any type) as well as a custom task which is to operate on a chunk of data the size of which is also provided. In addition, any required parameters that are global across all chunks are given. If the custom task needs to use data points which are not contained in the chunk it is to operate on (for instance for boundary conditions) then these can be set in the custom task definition. Finally, the ComputeClient will send the class files for the custom task implementation (which must be a subclass of UnitTask) and any required classes that are not in the standard Java library.

Once all this information has been sent to the master node, the GenericProcessor class takes care of mapping the data to the workers. First, the data sequence is divided into chunks of equal size (specified by client). Each chunk is packaged into a TaskWrapper object with an instance of the provided UnitTask subclass. Then, the TaskWrapper instances are mapped into worker task queues, one per worker, and if each chunk needs access to external data points, these are given to its TaskWrapper. Once the task queues have been prepared, a dedicated thread is started for each worker (implementation in WorkerLinkThread) and this sends the task queues to its worker.

When a task queue is received on a worker machine, a new thread is started which calls execute() on each TaskWrapper, which in turn puts result units on a result queue. As this is happening, the main thread takes result units from the queue and sends them back to the appropriate WorkerLinkThread on the master machine. Note that all queues used are instances of SynchronizedQueue, a thread-safe implementation of the Java LinkedBlockingQueue.

# How the ChainSpace works
On reception at the master, each worker thread dynamically puts its result units into the chain space which sequences result units on the fly. The chain space is a thread-safe data structure in which the following happens:
    1) the main thread in the master instantiates the chain space
    2) worker threads put their result units into the chain space, along with the position of each unit, encapsulated as Nodes
    3) when a node is inserted, it forms link(s) with one or both of the nodes before and after it respectively
    4) the chain space receives a request from the main thread to get a buffer of the next available result units
    5) if the next required node in the sequence is present it, along with any nodes it is linked to, is sent out through the chain          space's port
    6) if the next required node in the sequence is not present, the main thread blocks until it is
    7) when the final node is passed in, it forms a link with the 'tail' which, when received by the port, signals that all data has         been sequenced and that the chain space can be closed by the main thread.
    
The chain space is the central component of this framework as it facilitates the reordering of result units as they arrive, taking into account that they are not expected to arrive in the sequence in which they were sent out due to the unpredictable nature of processing times and the network. It preserves correctness by waiting for the next required node to arrive and efficiency by linking together other nodes which have arrived early in the meantime. This linking into chains can be useful for speed, especially if the project is extended to support storing arriving nodes in a database to improve scalability. In this case, the number of queries could be reduced substantially.

The client requests result units in a loop until they have all been received. When the master receives a result request it requests a result buffer from the chain space and sends the next unit to the client, storing the other units from the buffer in a temporary result queue until another result request comes in from the client.

ComputeClient returns result units to the calling program where they can be processed to the user's liking. For example, the provided sample executes a Simpson's Rule integration on several chunks of input data (x-values) and then cumulatively adds up the result units as they arrive so that the final integral is obtained when the last result unit arrives.
